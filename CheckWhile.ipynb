{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohaib/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import facenet\n",
    "import detect_face\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "import pdb\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('/home/sohaib/Desktop/Tools/real-time-deep-face-recognition/models')\n",
    "model_files = list(PATH.iterdir())\n",
    "lenm = len(model_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different length\n"
     ]
    }
   ],
   "source": [
    "while len(list(PATH.iterdir())) != lenm:\n",
    "    print('same length')\n",
    "    classifier_filename = '/home/sohaib/Desktop/Tools/real-time-deep-face-recognition/models/med_soh_kev.pkl'\n",
    "    lenm = len(model_files)\n",
    "    \n",
    "print('different length')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating networks and loading parameters\n",
      "Loading feature extraction model\n",
      "Model filename: /home/sohaib/Desktop/Tools/real-time-deep-face-recognition/models/20170512-110547/20170512-110547.pb\n",
      "load classifier file-> /home/sohaib/Desktop/Tools/real-time-deep-face-recognition/models/six20180522.pkl\n",
      "Start Recognition!\n",
      "Detected_FaceNum: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohaib/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:100: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78973788 0.03214635 0.06845445 0.03746935 0.01475746 0.0574345 ]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.75496756 0.03506463 0.07384829 0.05323644 0.01633349 0.06654959]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.76955462 0.03381538 0.06779062 0.05731378 0.01763925 0.05388636]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.73074699 0.03618651 0.07380819 0.07933111 0.02278519 0.05714201]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.71696113 0.03610477 0.07949601 0.08097981 0.02444276 0.06201552]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.78374015 0.03127357 0.06967779 0.0426591  0.01382445 0.05882493]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.79474753 0.02945885 0.07406867 0.02849763 0.01139234 0.061835  ]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.79505762 0.03149036 0.06577746 0.04101351 0.01350597 0.05315509]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.77170914 0.03382275 0.07182153 0.04882603 0.01486783 0.05895273]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.78310866 0.03217706 0.07093834 0.04308285 0.01261132 0.05808177]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.77890871 0.03308961 0.06388608 0.05163472 0.01931336 0.05316752]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.78695849 0.0341491  0.06793292 0.04143685 0.01495851 0.05456413]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.77385034 0.03454535 0.07020819 0.04714439 0.01507384 0.05917788]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.77767256 0.03345754 0.0643022  0.04977685 0.01679566 0.0579952 ]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.80490499 0.02813469 0.06362063 0.03852339 0.01313899 0.05167731]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.79304675 0.03110399 0.06342592 0.04762586 0.0157161  0.04908138]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.77392091 0.03391753 0.06640195 0.05330123 0.01884083 0.05361755]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.74634975 0.037771   0.06945569 0.06147777 0.01966746 0.06527833]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.75026687 0.0367324  0.0707778  0.05881081 0.02123008 0.06218204]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.76863453 0.03632979 0.06917374 0.05304661 0.0163816  0.05643374]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.72494852 0.03419164 0.0855389  0.07077101 0.0237956  0.06075432]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.67627224 0.03888954 0.09033937 0.10202808 0.02801206 0.06445871]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.64070733 0.03966814 0.10338007 0.11158221 0.03073092 0.07393134]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.67526885 0.04339313 0.10233324 0.08293596 0.02471345 0.07135538]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.67231465 0.04346533 0.09931934 0.09088736 0.0247523  0.06926102]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.67482283 0.04153557 0.09852477 0.09666143 0.02673145 0.06172395]]\n",
      "different length\n",
      "load classifier file-> /home/sohaib/Desktop/Tools/real-time-deep-face-recognition/models/med_soh_kev.pkl\n",
      "Detected_FaceNum: 1\n",
      "[[0.45337973 0.34337521 0.20324507]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.4310407  0.36103349 0.20792581]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.42194144 0.37825773 0.19980083]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.43262268 0.37084458 0.19653274]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.41523015 0.38226845 0.2025014 ]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.45087484 0.32911632 0.22000884]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.49297879 0.31757247 0.18944875]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.40242276 0.38431451 0.21326273]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.4162334  0.36073888 0.22302771]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.50540963 0.23680596 0.25778442]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.51972939 0.22582857 0.25444204]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.50232628 0.24697155 0.25070218]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.46284577 0.27196361 0.26519062]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.51532426 0.21442914 0.2702466 ]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.54459667 0.21487594 0.24052739]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.49251229 0.24121472 0.26627299]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.49435672 0.2478086  0.25783469]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.43716832 0.31739968 0.245432  ]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.42408962 0.33799224 0.23791814]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.40860183 0.34135461 0.25004356]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.40263369 0.34655801 0.2508083 ]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.41205594 0.34904829 0.23889577]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.47063064 0.29931098 0.23005837]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.44574558 0.30852641 0.24572801]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.41411155 0.34142183 0.24446662]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.43756051 0.34565874 0.21678075]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.45873316 0.31925221 0.22201464]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.40516554 0.34881569 0.24601877]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.42686025 0.33378924 0.23935051]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.45328083 0.28402859 0.26269059]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.49229143 0.2610655  0.24664307]]\n",
      "Detected_FaceNum: 1\n",
      "[[0.48115569 0.26646278 0.25238153]]\n",
      "Detected_FaceNum: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-931f2af966ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0mscaled_reshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_image_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mimages_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscaled_reshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase_train_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                         \u001b[0memb_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Creating networks and loading parameters')\n",
    "with tf.Graph().as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
    "    # sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n",
    "    sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "    with sess.as_default():\n",
    "        pnet, rnet, onet = detect_face.create_mtcnn(sess, '/home/sohaib/Desktop/Tools/real-time-deep-face-recognition/det_facenet')\n",
    "\n",
    "        minsize = 20  # minimum size of face\n",
    "        threshold = [0.6, 0.7, 0.7]  # three steps's threshold\n",
    "        factor = 0.709  # scale factor\n",
    "        margin = 44\n",
    "        frame_interval = 3\n",
    "        batch_size = 1000\n",
    "        image_size = 182\n",
    "        input_image_size = 160\n",
    "\n",
    "        HumanNames = ['Kevin', 'Med', 'Sohaib', 'Alessa', 'Kaori', 'Nicola']    # train human name\n",
    "        HumanNames = sorted(HumanNames)\n",
    "\n",
    "        print('Loading feature extraction model')\n",
    "        modeldir = '/home/sohaib/Desktop/Tools/real-time-deep-face-recognition/models/20170512-110547/20170512-110547.pb'\n",
    "        facenet.load_model(modeldir)\n",
    "\n",
    "        images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "        embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "        phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "        embedding_size = embeddings.get_shape()[1]\n",
    "\n",
    "        classifier_filename = '/home/sohaib/Desktop/Tools/real-time-deep-face-recognition/models/six20180522.pkl'\n",
    "        classifier_filename_exp = os.path.expanduser(classifier_filename)\n",
    "        with open(classifier_filename_exp, 'rb') as infile:\n",
    "            (model, class_names) = pickle.load(infile)\n",
    "            print('load classifier file-> %s' % classifier_filename_exp)\n",
    "\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        c = 0\n",
    "\n",
    "        # #video writer\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "        # out = cv2.VideoWriter('3F_0726.avi', fourcc, fps=30, frameSize=(640,480))\n",
    "\n",
    "        print('Start Recognition!')\n",
    "        prevTime = 0\n",
    "        while True:\n",
    "\n",
    "            while len(list(PATH.iterdir())) != lenm:\n",
    "                print('different length')\n",
    "                classifier_filename = '/home/sohaib/Desktop/Tools/real-time-deep-face-recognition/models/med_soh_kev.pkl'\n",
    "                lenm = len(list(PATH.iterdir()))\n",
    "                \n",
    "                classifier_filename_exp = os.path.expanduser(classifier_filename)\n",
    "                with open(classifier_filename_exp, 'rb') as infile:\n",
    "                    (model, class_names) = pickle.load(infile)\n",
    "                    print('load classifier file-> %s' % classifier_filename_exp)\n",
    "                break;\n",
    "            \n",
    "            ret, frame = video_capture.read()\n",
    "\n",
    "            frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)    # resize frame (optional)\n",
    "\n",
    "            curTime = time.time()    # calc fps\n",
    "            timeF = frame_interval\n",
    "\n",
    "            if c % timeF == 0:\n",
    "                find_results = []\n",
    "\n",
    "                if frame.ndim == 2:\n",
    "                    frame = facenet.to_rgb(frame)\n",
    "                frame = frame[:, :, 0:3]\n",
    "\n",
    "                bounding_boxes, _ = detect_face.detect_face(frame, minsize, pnet, rnet, onet, threshold, factor)\n",
    "                nrof_faces = bounding_boxes.shape[0]\n",
    "                print('Detected_FaceNum: %d' % nrof_faces)\n",
    "\n",
    "                if nrof_faces > 0:\n",
    "                    det = bounding_boxes[:, 0:4]\n",
    "                    img_size = np.asarray(frame.shape)[0:2]\n",
    "\n",
    "                    cropped = []\n",
    "                    scaled = []\n",
    "                    scaled_reshape = []\n",
    "                    bb = np.zeros((nrof_faces,4), dtype=np.int32)\n",
    "\n",
    "                    for i in range(nrof_faces):\n",
    "                        emb_array = np.zeros((1, embedding_size))\n",
    "\n",
    "                        bb[i][0] = det[i][0]\n",
    "                        bb[i][1] = det[i][1]\n",
    "                        bb[i][2] = det[i][2]\n",
    "                        bb[i][3] = det[i][3]\n",
    "\n",
    "                        # inner exception\n",
    "                        if bb[i][0] <= 0 or bb[i][1] <= 0 or bb[i][2] >= len(frame[0]) or bb[i][3] >= len(frame):\n",
    "                            print('face is inner of range!')\n",
    "                            continue\n",
    "\n",
    "                        cropped.append(frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :])\n",
    "                        cropped[i] = facenet.flip(cropped[i], False)\n",
    "                        scaled.append(misc.imresize(cropped[i], (image_size, image_size), interp='bilinear'))\n",
    "                        scaled[i] = cv2.resize(scaled[i], (input_image_size, input_image_size),\n",
    "                                               interpolation=cv2.INTER_CUBIC)\n",
    "                        scaled[i] = facenet.prewhiten(scaled[i])\n",
    "                        scaled_reshape.append(scaled[i].reshape(-1, input_image_size, input_image_size, 3))\n",
    "                        feed_dict = {images_placeholder: scaled_reshape[i], phase_train_placeholder: False}\n",
    "                        emb_array[0, :] = sess.run(embeddings, feed_dict=feed_dict)\n",
    "                        predictions = model.predict_proba(emb_array)\n",
    "                        print(predictions)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if new weights where added in the folder\n",
    "while len(list(PATH.iterdir())) != initial_folder_length:\n",
    "    print('different length')\n",
    "    classifier_filename = 'last_weights.pkl'\n",
    "    initial_folder_length = len(list(PATH.iterdir()))\n",
    "\n",
    "    # load the new weights inside the model\n",
    "    classifier_filename_exp = os.path.expanduser(classifier_filename)\n",
    "    with open(classifier_filename_exp, 'rb') as infile:\n",
    "        (model, class_names) = pickle.load(infile)\n",
    "        print('load classifier file-> %s' % classifier_filename_exp)\n",
    "    break;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
